{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPELINE DE DADOS - A1 - APLICANDO CONHECIMENTO\n",
    "## Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import glob\n",
    "import psycopg2\n",
    "from io import StringIO\n",
    "import csv\n",
    "from sqlalchemy import create_engine, text\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação de funções para inserir dados no Banco de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lista_url(dias):\n",
    "    # Data de hoje\n",
    "    data_atual = datetime.now()\n",
    "\n",
    "    # Lista para armazenar os nomes dos arquivos\n",
    "    lista = []\n",
    "\n",
    "    # Gerar uma lista com x dias a partir da variável dias\n",
    "    for i in range(dias):\n",
    "        # Calcular a data correspondente\n",
    "        data = data_atual - timedelta(days=i)\n",
    "        # Formatando no estilo 'YYYYMMDD'\n",
    "        data_formatada = data.strftime('%Y%m%d')\n",
    "        # Criar o nome do arquivo\n",
    "        nome_arquivo = f'https://dataserver-coids.inpe.br/queimadas/queimadas/focos/csv/diario/Brasil/focos_diario_br_{data_formatada}.csv'\n",
    "        # Adicionar à lista\n",
    "        lista.append(nome_arquivo)\n",
    "\n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(urls, storage):\n",
    "    \n",
    "    # Cria a pasta 'storage' se ela não existir\n",
    "    if not os.path.exists(storage):\n",
    "        os.makedirs(storage)\n",
    "    \n",
    "    # Itera sobre as URLs para fazer o download dos arquivos\n",
    "    for url in urls:\n",
    "        try:\n",
    "            # Extrai o nome do arquivo da URL\n",
    "            file_name = url.split('/')[-1]\n",
    "            \n",
    "            # Caminho completo para salvar o arquivo\n",
    "            file_path = os.path.join(storage, file_name)\n",
    "            \n",
    "            # Faz o download do arquivo\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Verifica se houve erros na solicitação\n",
    "            \n",
    "            # Salva o arquivo na pasta 'storage'\n",
    "            with open(file_path, 'wb') as arquivo:\n",
    "                arquivo.write(response.content)\n",
    "                \n",
    "            print(f\"Arquivo {file_name} salvo com sucesso em 'storage'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao baixar {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_df(storage):\n",
    "    # Criação de uma lista com os arquivos csv\n",
    "    csv_files = glob.glob(f'{storage}/*.csv')\n",
    "\n",
    "    # Definição do schema do arquivo csv\n",
    "    schema = {\n",
    "        'id': 'object',\n",
    "        'lat': 'object',\n",
    "        'lon': 'object',\n",
    "        'data_hora_gmt': 'object',\n",
    "        'satelite' : 'object',\n",
    "        'municipio': 'object',\n",
    "        'estado': 'object',\n",
    "        'pais': 'object',\n",
    "        'municipio_id': 'object',\n",
    "        'estado_id' : 'object',\n",
    "        'pais_id': 'object',\n",
    "        'numero_dias_sem_chuva': 'object',\n",
    "        'precipitacao': 'object',\n",
    "        'risco_fogo': 'object',\n",
    "        'bioma': 'object',\n",
    "        'frp': 'object'\n",
    "    }\n",
    "    # Criação de um dataframe vazio\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # Percorre todos os arquivos na pasta\n",
    "    for csv_file in csv_files:\n",
    "        df_csv = pd.read_csv(csv_file, dtype=schema, delimiter=',')\n",
    "        df = pd.concat([df, df_csv])\n",
    "    \n",
    "    # Trata o dataframe para substituir os NAN por null\n",
    "    df = df.replace({pd.NA: None})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connection_postgres():\n",
    "    # Definir os parâmetros de conexão com o PostgreSQL\n",
    "    conn = psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        port=\"15432\",\n",
    "        database=\"mack\", \n",
    "        user=\"mack\",\n",
    "        password=\"mack\"\n",
    "    )\n",
    "\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_db():\n",
    "    # Variável para se conectar ao postgres\n",
    "    conn = get_connection_postgres()\n",
    "\n",
    "    # Criação de um cursor para executar comandos SQL\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Definir o comando SQL para criar a tabela\n",
    "    criar_tabela_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS focos_queimadas (\n",
    "        id TEXT,\n",
    "        lat TEXT,\n",
    "        lon TEXT,\n",
    "        data_hora_gmt TEXT,\n",
    "        satelite TEXT,\n",
    "        municipio TEXT,\n",
    "        estado TEXT,\n",
    "        pais TEXT,\n",
    "        municipio_id TEXT,\n",
    "        estado_id TEXT,\n",
    "        pais_id TEXT,\n",
    "        numero_dias_sem_chuva TEXT,\n",
    "        precipitacao TEXT,\n",
    "        risco_fogo TEXT,\n",
    "        bioma TEXT,\n",
    "        frp TEXT\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Executar o comando SQL\n",
    "        cursor.execute(criar_tabela_sql)\n",
    "        conn.commit()  # Confirma a transação no banco de dados\n",
    "        print(\"Tabela criada com sucesso!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao criar a tabela: {e}\")\n",
    "        conn.rollback()  # Reverte a transação em caso de erro\n",
    "    finally:\n",
    "        # Fechar a conexão com o banco de dados\n",
    "        cursor.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inserir_dados_db(df, table):\n",
    "    df_inclusao = df.copy()\n",
    "    \n",
    "    # Verificar se o DataFrame tem exatamente 16 colunas\n",
    "    expected_columns = 16\n",
    "    if df_inclusao.shape[1] != expected_columns:\n",
    "        raise ValueError(f\"O DataFrame deve ter exatamente {expected_columns} colunas.\")\n",
    "    \n",
    "    # Variável para se conectar ao postgres\n",
    "    conn = get_connection_postgres()\n",
    "\n",
    "    sio = StringIO()\n",
    "    writer = csv.writer(sio)\n",
    "    writer.writerows(df_inclusao.values)\n",
    "    sio.seek(0)\n",
    "    try:\n",
    "        with conn.cursor() as c:\n",
    "            c.copy_expert(\n",
    "                sql=f\"\"\"\n",
    "                COPY {table} (\n",
    "                    id,\n",
    "                    lat,\n",
    "                    lon,\n",
    "                    data_hora_gmt,\n",
    "                    satelite,\n",
    "                    municipio,\n",
    "                    estado,\n",
    "                    pais,\n",
    "                    municipio_id,\n",
    "                    estado_id,\n",
    "                    pais_id,\n",
    "                    numero_dias_sem_chuva,\n",
    "                    precipitacao,\n",
    "                    risco_fogo,\n",
    "                    bioma,\n",
    "                    frp\n",
    "                ) FROM STDIN WITH CSV\"\"\",\n",
    "                file=sio\n",
    "            )\n",
    "            conn.commit()\n",
    "            print('Dados inseridos na tabela com sucesso!')\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao inserir dados: {e}\")\n",
    "        conn.rollback()  # Reverte a transação em caso de erro\n",
    "    finally:\n",
    "        # Fechar a conexão com o banco de dados\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excluir_linhas_db(df, table):\n",
    "    df_datas = df.copy()\n",
    "\n",
    "    # Extraindo as datas distintas da coluna 'data_hora_gmt' no formato YYYY-MM-DD\n",
    "    df_datas['data'] = df_datas['data_hora_gmt'].str[:10]  # Mantém apenas a data (YYYY-MM-DD)\n",
    "    datas_distintas = df_datas['data'].unique()  # Obtém os valores únicos de data\n",
    "\n",
    "    # Conectar ao banco de dados PostgreSQL usando SQLAlchemy\n",
    "    engine = create_engine('postgresql+psycopg2://mack:mack@localhost:15432/mack')\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            # Executar uma query DELETE para cada data distinta\n",
    "            for data in datas_distintas:\n",
    "                query_delete = text(f\"DELETE FROM {table} WHERE data_hora_gmt::text LIKE '{data}%';\")\n",
    "                conn.execute(query_delete)\n",
    "                print(f\"Linha(s) com data {data} excluída(s) com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao excluir as linhas: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excluir_arquivos(storage):\n",
    "    \"\"\"\n",
    "    Remove todos os arquivos de um diretório especificado.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): O caminho do diretório onde os arquivos serão excluídos.\n",
    "    \"\"\"\n",
    "     # Verificar se o diretório existe\n",
    "    if not os.path.isdir(storage):\n",
    "        print(f\"O caminho especificado não é um diretório válido: {storage}\")\n",
    "        return\n",
    "\n",
    "    # Listar todos os arquivos e pastas no diretório\n",
    "    for nome_arquivo in os.listdir(storage):\n",
    "        caminho_arquivo = os.path.join(storage, nome_arquivo)\n",
    "        \n",
    "        try:\n",
    "            os.remove(caminho_arquivo)  # Excluir o arquivo\n",
    "            print(f\"Arquivo {caminho_arquivo} excluído com sucesso.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao excluir o arquivo {caminho_arquivo}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chama as funções de forma ordenada para excluir os dados antigos e inserir novos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela criada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Chamar a função para criar a tabela\n",
    "create_table_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo /home/yuri/mack/src/../storage/focos_diario_br_20240902.csv excluído com sucesso.\n",
      "Arquivo /home/yuri/mack/src/../storage/focos_diario_br_20240904.csv excluído com sucesso.\n",
      "Arquivo /home/yuri/mack/src/../storage/focos_diario_br_20240901.csv excluído com sucesso.\n",
      "Arquivo /home/yuri/mack/src/../storage/focos_diario_br_20240831.csv excluído com sucesso.\n",
      "Arquivo /home/yuri/mack/src/../storage/focos_diario_br_20240830.csv excluído com sucesso.\n",
      "Arquivo /home/yuri/mack/src/../storage/focos_diario_br_20240905.csv excluído com sucesso.\n",
      "Arquivo /home/yuri/mack/src/../storage/focos_diario_br_20240903.csv excluído com sucesso.\n",
      "Arquivo /home/yuri/mack/src/../storage/focos_diario_br_20240908.csv excluído com sucesso.\n",
      "Arquivo /home/yuri/mack/src/../storage/focos_diario_br_20240906.csv excluído com sucesso.\n",
      "Arquivo /home/yuri/mack/src/../storage/focos_diario_br_20240907.csv excluído com sucesso.\n",
      "Arquivo focos_diario_br_20240908.csv salvo com sucesso em 'storage'.\n",
      "Arquivo focos_diario_br_20240907.csv salvo com sucesso em 'storage'.\n",
      "Arquivo focos_diario_br_20240906.csv salvo com sucesso em 'storage'.\n",
      "Arquivo focos_diario_br_20240905.csv salvo com sucesso em 'storage'.\n",
      "Arquivo focos_diario_br_20240904.csv salvo com sucesso em 'storage'.\n",
      "Arquivo focos_diario_br_20240903.csv salvo com sucesso em 'storage'.\n",
      "Arquivo focos_diario_br_20240902.csv salvo com sucesso em 'storage'.\n",
      "Arquivo focos_diario_br_20240901.csv salvo com sucesso em 'storage'.\n",
      "Arquivo focos_diario_br_20240831.csv salvo com sucesso em 'storage'.\n",
      "Arquivo focos_diario_br_20240830.csv salvo com sucesso em 'storage'.\n"
     ]
    }
   ],
   "source": [
    "# Gera a lista de urls\n",
    "lista_urls = get_lista_url(10)\n",
    "\n",
    "# Define o caminho completo da pasta 'storage'\n",
    "storage = os.path.join(os.getcwd(), '..', 'storage')\n",
    "\n",
    "# Chama a função para excluir os arquivos CSV já baixados\n",
    "excluir_arquivos(storage)\n",
    "\n",
    "# Chama a função para fazer os downloads do arquivos\n",
    "download_files(lista_urls, storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linha(s) com data 2024-09-02 excluída(s) com sucesso.\n",
      "Linha(s) com data 2024-09-04 excluída(s) com sucesso.\n",
      "Linha(s) com data 2024-09-01 excluída(s) com sucesso.\n",
      "Linha(s) com data 2024-08-31 excluída(s) com sucesso.\n",
      "Linha(s) com data 2024-08-30 excluída(s) com sucesso.\n",
      "Linha(s) com data 2024-09-05 excluída(s) com sucesso.\n",
      "Linha(s) com data 2024-09-03 excluída(s) com sucesso.\n",
      "Linha(s) com data 2024-09-08 excluída(s) com sucesso.\n",
      "Linha(s) com data 2024-09-06 excluída(s) com sucesso.\n",
      "Linha(s) com data 2024-09-07 excluída(s) com sucesso.\n",
      "Dados inseridos na tabela com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Gera um dataframe do pandas a partir dos arquivos CSV\n",
    "df = csv_to_df(storage)\n",
    "\n",
    "# Chama a função para excluir os dados já existes para não ter duplicados\n",
    "excluir_linhas_db(df, 'focos_queimadas')\n",
    "\n",
    "# Chama a função para inserir os dados no DB do Postgres\n",
    "inserir_dados_db(df, 'focos_queimadas')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
